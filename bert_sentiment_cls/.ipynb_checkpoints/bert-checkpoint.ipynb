{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "dmAv8xZOZXV1",
    "outputId": "667ee3c1-5b15-435e-fe82-b267e0becdf4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained: str,\n",
    "                 num_classes=3,\n",
    "                 pooling_output_layer=-1):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 768, num_classes\n",
    "        self.bert = BertModel.from_pretrained(pretrained)\n",
    "        self.classifier = nn.Sequential(nn.Linear(D_in, H), nn.Tanh(), nn.Linear(H, D_out))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.pooling_output_layer = pooling_output_layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, output_hidden_states=True):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_hidden_states=output_hidden_states)\n",
    "        sentence_embeddings = outputs[1]\n",
    "        sentence_embeddings = self.dropout(sentence_embeddings)\n",
    "        logits = self.classifier(sentence_embeddings.to(device))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path=\"save_model\"\n",
    "final_model_path=\"final_model\"\n",
    "max_sequence_length=128\n",
    "batch_size=64\n",
    "epochs=3,\n",
    "warmup_steps=2000\n",
    "lr=3e-5\n",
    "max_grad_norm=1.0\n",
    "log_step=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_list, label_list, tokenizer, max_sequence_len):\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.label_list = label_list\n",
    "        self.len = len(label_list)\n",
    "        for text in tqdm(text_list):\n",
    "            text = text[:max_sequence_len - 2]\n",
    "            title_ids = tokenizer.encode_plus(text, padding='max_length', max_length=max_sequence_len)\n",
    "            self.input_ids.append(title_ids['input_ids'])\n",
    "            self.attention_mask.append(title_ids[\"attention_mask\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tmp_input_ids = self.input_ids[index]\n",
    "        tmp_attention_mask = self.attention_mask[index]\n",
    "        tmp_label = self.label_list[index]\n",
    "        output = {\"input_ids\": torch.tensor(tmp_input_ids).to(device),\n",
    "                  \"attention_mask\": torch.tensor(tmp_attention_mask).to(device)}\n",
    "        return output, tmp_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(x_list, y_list, tokenizer, max_sequence_len, batch_size, shuffle):\n",
    "    dataset = MyDataset(x_list, y_list, tokenizer, max_sequence_len)\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_labels):\n",
    "    model = BertClassifier(\"bert-base-uncased\", num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, label):\n",
    "    predicted_class_id = torch.tensor([w.argmax().item() for w in logits])\n",
    "    return float((predicted_class_id == label).float().sum()) / label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label_dict = {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}\n",
    "def load_raw_data(data_path):\n",
    "    # 读取原始数据，原始数据很多，抽取前5万行训练，5000行验证\n",
    "    train_x, train_y = [], []\n",
    "    eval_x, eval_y = [], []\n",
    "    df = pd.read_csv(data_path)\n",
    "    for idx in range(len(df)):\n",
    "        if idx < 1300000:\n",
    "            train_x.append(df['review_text'][idx])\n",
    "            train_y.append(label_dict[int(df['rating'][idx])])\n",
    "        else:\n",
    "            eval_x.append(df['review_text'][idx])\n",
    "            eval_y.append(label_dict[int(df['rating'][idx])])\n",
    "    print(len(train_x), len(train_y))\n",
    "    print(len(eval_x), len(eval_y))\n",
    "    return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, dataloader, device):\n",
    "    num_training_steps = epochs * len(dataloader)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    batch_steps = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for batch, label in dataloader:\n",
    "            batch_steps += 1\n",
    "            logits = model(**batch)\n",
    "            acc = compute_acc(logits, label)\n",
    "            loss = loss_fct(logits.view(-1, 2).to(device), label.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if batch_steps % log_step == 0:\n",
    "                print(\"train epoch {}/{}, batch {}/{}, loss {}, acc {}\".format(\n",
    "                    epoch + 1, args.epochs,\n",
    "                    batch_steps,\n",
    "                    num_training_steps,\n",
    "                    loss,\n",
    "                    acc))\n",
    "    torch.save(model, 'model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300000 1300000\n",
      "187747 187747\n",
      "训练数据 1300000\n",
      "验证数据 187747\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8dcf7e8b8cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                                \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(num_labels=3)\n",
    "train_x, train_y, eval_x, eval_y = load_raw_data(\"./data/review.csv\")\n",
    "print(\"训练数据\", len(train_x))\n",
    "print(\"验证数据\", len(eval_x))\n",
    "train_dataloader = data_loader(train_x,\n",
    "                               train_y,\n",
    "                               tokenizer,\n",
    "                               max_sequence_length,\n",
    "                               batch_size,\n",
    "                               True)\n",
    "train(args, model, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
