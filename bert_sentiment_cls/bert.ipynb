{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "dmAv8xZOZXV1",
    "outputId": "667ee3c1-5b15-435e-fe82-b267e0becdf4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained: str,\n",
    "                 num_classes=3,\n",
    "                 pooling_output_layer=-1):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 768, num_classes\n",
    "        self.bert = BertModel.from_pretrained(pretrained)\n",
    "        self.classifier = nn.Sequential(nn.Linear(D_in, H), nn.Tanh(), nn.Linear(H, D_out))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.pooling_output_layer = pooling_output_layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, output_hidden_states=True):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_hidden_states=output_hidden_states)\n",
    "        sentence_embeddings = outputs[1]\n",
    "        sentence_embeddings = self.dropout(sentence_embeddings)\n",
    "        logits = self.classifier(sentence_embeddings.to(device))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length=130\n",
    "batch_size=128\n",
    "epochs=2\n",
    "warmup_steps=2000\n",
    "lr=3e-5\n",
    "max_grad_norm=1.0\n",
    "log_step=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_list, label_list, tokenizer, max_sequence_len):\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.label_list = label_list\n",
    "        self.len = len(label_list)\n",
    "        for text in tqdm(text_list):\n",
    "            text = text[:max_sequence_len - 2]\n",
    "            title_ids = tokenizer.encode_plus(text, padding='max_length', max_length=max_sequence_len, truncation=True)\n",
    "            self.input_ids.append(title_ids['input_ids'])\n",
    "            self.attention_mask.append(title_ids[\"attention_mask\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tmp_input_ids = self.input_ids[index]\n",
    "        tmp_attention_mask = self.attention_mask[index]\n",
    "        tmp_label = self.label_list[index]\n",
    "        output = {\"input_ids\": torch.tensor(tmp_input_ids).to(device),\n",
    "                  \"attention_mask\": torch.tensor(tmp_attention_mask).to(device)}\n",
    "        return output, tmp_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(x_list, y_list, tokenizer, max_sequence_len, batch_size, shuffle):\n",
    "    dataset = MyDataset(x_list, y_list, tokenizer, max_sequence_len)\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_labels):\n",
    "    model = BertClassifier(\"bert-base-uncased\", num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, label):\n",
    "    predicted_class_id = torch.tensor([w.argmax().item() for w in logits])\n",
    "    return float((predicted_class_id == label).float().sum()) / label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label_dict = {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}\n",
    "def load_raw_data(data_path):\n",
    "    # load raw data\n",
    "    train_x, train_y = [], []\n",
    "    eval_x, eval_y = [], []\n",
    "    df = pd.read_csv(data_path)\n",
    "    for idx in range(len(df)):\n",
    "        if idx < 1300000:\n",
    "            train_x.append(df['review_text'][idx])\n",
    "            train_y.append(label_dict[int(df['rating'][idx])])\n",
    "        else:\n",
    "            eval_x.append(df['review_text'][idx])\n",
    "            eval_y.append(label_dict[int(df['rating'][idx])])\n",
    "    print('训练数据:', len(train_x), len(train_y))\n",
    "    print('验证数据:', len(eval_x), len(eval_y))\n",
    "    return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, device):\n",
    "    num_training_steps = epochs * len(dataloader)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    batch_steps = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for batch, label in dataloader:\n",
    "            batch_steps += 1\n",
    "            logits = model(**batch)\n",
    "            acc = compute_acc(logits, label)\n",
    "            loss = loss_fct(logits.view(-1, 3).to(device), label.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if batch_steps % log_step == 0:\n",
    "                print(\"train epoch {}/{}, batch {}/{}, loss {}, acc {}\".format(\n",
    "                    epoch + 1, epochs,\n",
    "                    batch_steps,\n",
    "                    num_training_steps,\n",
    "                    loss,\n",
    "                    acc))\n",
    "    torch.save(model, 'model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(dataloader):\n",
    "    model = torch.load(\"model_final.pth\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    labels_all = []\n",
    "    predict_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch, label in dataloader:\n",
    "            labels_all.extend(label)\n",
    "            logits = model(**batch)\n",
    "            acc = compute_acc(logits, label)\n",
    "            loss = loss_fct(logits.view(-1, 3).to(device), label.view(-1).to(device))\n",
    "            loss_list.append(float(loss))\n",
    "            acc_list.append(float(acc))\n",
    "            predict_label = [w.argmax().item() for w in logits]\n",
    "            predict_all.extend(predict_label)\n",
    "    print(\"loss: {},\".format(np.mean(loss_list)),\n",
    "          \"accuracy: {}.\".format(np.mean(acc_list)))\n",
    "    labels_all = [w.item() for w in labels_all]\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average='weighted')\n",
    "    r = recall_score(labels_all, predict_all, average='weighted')\n",
    "    f1 = f1_score(labels_all, predict_all, average='weighted')\n",
    "    print(classification_report(labels_all, predict_all))\n",
    "    print('acc:', acc)\n",
    "    print('precision:', p)\n",
    "    print('recall:', r)\n",
    "    print('f1:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, text, tokenizer):\n",
    "    model = torch.load(\"model_final.pth\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    time_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        text = text[:max_sequence_length - 2]\n",
    "        inputs = tokenizer.encode_plus(text,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=max_sequence_length,\n",
    "                                       return_tensors=\"pt\")\n",
    "        inputs = {\"input_ids\": inputs['input_ids'].to(device),\n",
    "                  \"attention_mask\": inputs['attention_mask'].to(device)}\n",
    "        logits = model(**inputs)\n",
    "        print(\"predict time cost {}\".format(time.time() - time_start))\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "    print(\"text: {}\".format(text))\n",
    "    print(\"predict label: {}\".format(predicted_class_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据: 1300000 1300000\n",
      "验证数据: 187747 187747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25f9ac614b44383a85fce38da7040f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1300000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1/2, batch 500/20314, loss 0.29863736033439636, acc 0.890625\n",
      "train epoch 1/2, batch 1000/20314, loss 0.3169819712638855, acc 0.8671875\n",
      "train epoch 1/2, batch 1500/20314, loss 0.2742278277873993, acc 0.90625\n",
      "train epoch 1/2, batch 2000/20314, loss 0.23564806580543518, acc 0.90625\n",
      "train epoch 1/2, batch 2500/20314, loss 0.20182210206985474, acc 0.921875\n",
      "train epoch 1/2, batch 3000/20314, loss 0.1342770904302597, acc 0.96875\n",
      "train epoch 1/2, batch 3500/20314, loss 0.21181869506835938, acc 0.921875\n",
      "train epoch 1/2, batch 4000/20314, loss 0.20062929391860962, acc 0.9375\n",
      "train epoch 1/2, batch 4500/20314, loss 0.22458180785179138, acc 0.8984375\n",
      "train epoch 1/2, batch 5000/20314, loss 0.15703251957893372, acc 0.9453125\n",
      "train epoch 1/2, batch 5500/20314, loss 0.17514324188232422, acc 0.9453125\n",
      "train epoch 1/2, batch 6000/20314, loss 0.18991750478744507, acc 0.953125\n",
      "train epoch 1/2, batch 6500/20314, loss 0.1247263103723526, acc 0.953125\n",
      "train epoch 1/2, batch 7000/20314, loss 0.17374181747436523, acc 0.9375\n",
      "train epoch 1/2, batch 7500/20314, loss 0.27672624588012695, acc 0.90625\n",
      "train epoch 1/2, batch 8000/20314, loss 0.20359309017658234, acc 0.9375\n",
      "train epoch 1/2, batch 8500/20314, loss 0.1757829338312149, acc 0.9296875\n",
      "train epoch 1/2, batch 9000/20314, loss 0.16925513744354248, acc 0.9296875\n",
      "train epoch 1/2, batch 9500/20314, loss 0.25413575768470764, acc 0.90625\n",
      "train epoch 1/2, batch 10000/20314, loss 0.1930091381072998, acc 0.9296875\n",
      "train epoch 2/2, batch 10500/20314, loss 0.13170674443244934, acc 0.9609375\n",
      "train epoch 2/2, batch 11000/20314, loss 0.09570053964853287, acc 0.9609375\n",
      "train epoch 2/2, batch 11500/20314, loss 0.16945256292819977, acc 0.9453125\n",
      "train epoch 2/2, batch 12000/20314, loss 0.14337870478630066, acc 0.9609375\n",
      "train epoch 2/2, batch 12500/20314, loss 0.11437185853719711, acc 0.953125\n",
      "train epoch 2/2, batch 13000/20314, loss 0.22094185650348663, acc 0.9140625\n",
      "train epoch 2/2, batch 13500/20314, loss 0.3065692186355591, acc 0.859375\n",
      "train epoch 2/2, batch 14000/20314, loss 0.2518574297428131, acc 0.90625\n",
      "train epoch 2/2, batch 14500/20314, loss 0.1956816017627716, acc 0.9375\n",
      "train epoch 2/2, batch 15000/20314, loss 0.16908735036849976, acc 0.9453125\n",
      "train epoch 2/2, batch 15500/20314, loss 0.23816806077957153, acc 0.9296875\n",
      "train epoch 2/2, batch 16000/20314, loss 0.10541556030511856, acc 0.953125\n",
      "train epoch 2/2, batch 16500/20314, loss 0.2546309530735016, acc 0.9296875\n",
      "train epoch 2/2, batch 17000/20314, loss 0.20384420454502106, acc 0.9140625\n",
      "train epoch 2/2, batch 17500/20314, loss 0.13046757876873016, acc 0.9609375\n",
      "train epoch 2/2, batch 18000/20314, loss 0.23919273912906647, acc 0.9296875\n",
      "train epoch 2/2, batch 18500/20314, loss 0.18712478876113892, acc 0.9453125\n",
      "train epoch 2/2, batch 19000/20314, loss 0.34791049361228943, acc 0.8828125\n",
      "train epoch 2/2, batch 19500/20314, loss 0.1437414437532425, acc 0.9609375\n",
      "train epoch 2/2, batch 20000/20314, loss 0.20130552351474762, acc 0.9296875\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(num_labels=3)\n",
    "train_x, train_y, eval_x, eval_y = load_raw_data(\"./data/review.csv\")\n",
    "train_dataloader = data_loader(train_x,\n",
    "                               train_y,\n",
    "                               tokenizer,\n",
    "                               max_sequence_length,\n",
    "                               batch_size,\n",
    "                               True)\n",
    "train(model, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf50d427a80748f2abaab780169ae5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187747 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.19506543190120107, accuracy: 0.9313024157732747.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75     11049\n",
      "           1       0.54      0.36      0.43     10973\n",
      "           2       0.96      0.98      0.97    165725\n",
      "\n",
      "    accuracy                           0.93    187747\n",
      "   macro avg       0.75      0.70      0.72    187747\n",
      "weighted avg       0.92      0.93      0.93    187747\n",
      "\n",
      "acc: 0.9313011659307472\n",
      "precision: 0.9238351922014859\n",
      "recall: 0.9313011659307472\n",
      "f1: 0.926392549375741\n"
     ]
    }
   ],
   "source": [
    "# print(\" loss and acc\")\n",
    "eval_dataloader = data_loader(eval_x,\n",
    "                              eval_y,\n",
    "                              tokenizer,\n",
    "                              max_sequence_length,\n",
    "                              batch_size,\n",
    "                              False)\n",
    "evaluate(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单条预测～～～\n",
      "predict time cost 0.01199030876159668\n",
      "text: Great Dj & dance music, food needs a bit of improvement as it lacks flavor\n",
      "predict label: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"PREDICT \")\n",
    "text = \"Great Dj & dance music, food needs a bit of improvement as it lacks flavor\"\n",
    "predict(device, text, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
