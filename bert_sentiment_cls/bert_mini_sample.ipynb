{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "dmAv8xZOZXV1",
    "outputId": "667ee3c1-5b15-435e-fe82-b267e0becdf4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained: str,\n",
    "                 num_classes=3,\n",
    "                 pooling_output_layer=-1):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 768, num_classes\n",
    "        self.bert = BertModel.from_pretrained(pretrained)\n",
    "        self.classifier = nn.Sequential(nn.Linear(D_in, H), nn.Tanh(), nn.Linear(H, D_out))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.pooling_output_layer = pooling_output_layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, output_hidden_states=True):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_hidden_states=output_hidden_states)\n",
    "        sentence_embeddings = outputs[1]\n",
    "        sentence_embeddings = self.dropout(sentence_embeddings)\n",
    "        logits = self.classifier(sentence_embeddings.to(device))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length=130\n",
    "batch_size=128\n",
    "epochs=2\n",
    "warmup_steps=2000\n",
    "lr=3e-5\n",
    "max_grad_norm=1.0\n",
    "log_step=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_list, label_list, tokenizer, max_sequence_len):\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.label_list = label_list\n",
    "        self.len = len(label_list)\n",
    "        for text in tqdm(text_list):\n",
    "            text = text[:max_sequence_len - 2]\n",
    "            title_ids = tokenizer.encode_plus(text, padding='max_length', max_length=max_sequence_len, truncation=True)\n",
    "            self.input_ids.append(title_ids['input_ids'])\n",
    "            self.attention_mask.append(title_ids[\"attention_mask\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tmp_input_ids = self.input_ids[index]\n",
    "        tmp_attention_mask = self.attention_mask[index]\n",
    "        tmp_label = self.label_list[index]\n",
    "        output = {\"input_ids\": torch.tensor(tmp_input_ids).to(device),\n",
    "                  \"attention_mask\": torch.tensor(tmp_attention_mask).to(device)}\n",
    "        return output, tmp_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(x_list, y_list, tokenizer, max_sequence_len, batch_size, shuffle):\n",
    "    dataset = MyDataset(x_list, y_list, tokenizer, max_sequence_len)\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(num_labels):\n",
    "    model = BertClassifier(\"bert-base-uncased\", num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, label):\n",
    "    predicted_class_id = torch.tensor([w.argmax().item() for w in logits])\n",
    "    return float((predicted_class_id == label).float().sum()) / label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label_dict = {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}\n",
    "def load_raw_data(data_path):\n",
    "    # load raw data ，200000line for train and scale the test set\n",
    "    train_x, train_y = [], []\n",
    "    eval_x, eval_y = [], []\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['label'] = df['rating'].replace(label_dict)\n",
    "    use_df = pd.concat([df[df.label!=2], df[df.label==2].sample(100000)])\n",
    "    use_df = use_df.sample(frac=1.0).reset_index()\n",
    "    for idx in range(len(use_df)):\n",
    "        if idx < 200000:\n",
    "            train_x.append(use_df['review_text'][idx])\n",
    "            train_y.append(int(use_df['label'][idx]))\n",
    "        else:\n",
    "            eval_x.append(use_df['review_text'][idx])\n",
    "            eval_y.append(int(use_df['label'][idx]))\n",
    "    print('训练数据:', len(train_x), len(train_y))\n",
    "    print('验证数据:', len(eval_x), len(eval_y))\n",
    "    return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, device):\n",
    "    num_training_steps = epochs * len(dataloader)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    batch_steps = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for batch, label in dataloader:\n",
    "            batch_steps += 1\n",
    "            logits = model(**batch)\n",
    "            acc = compute_acc(logits, label)\n",
    "            loss = loss_fct(logits.view(-1, 3).to(device), label.view(-1).to(device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if batch_steps % log_step == 0:\n",
    "                print(\"train epoch {}/{}, batch {}/{}, loss {}, acc {}\".format(\n",
    "                    epoch + 1, epochs,\n",
    "                    batch_steps,\n",
    "                    num_training_steps,\n",
    "                    loss,\n",
    "                    acc))\n",
    "    torch.save(model, 'bert_mini_sample_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(dataloader):\n",
    "    model = torch.load(\"bert_mini_sample_model.pth\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    labels_all = []\n",
    "    predict_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch, label in dataloader:\n",
    "            labels_all.extend(label)\n",
    "            logits = model(**batch)\n",
    "            acc = compute_acc(logits, label)\n",
    "            loss = loss_fct(logits.view(-1, 3).to(device), label.view(-1).to(device))\n",
    "            loss_list.append(float(loss))\n",
    "            acc_list.append(float(acc))\n",
    "            predict_label = [w.argmax().item() for w in logits]\n",
    "            predict_all.extend(predict_label)\n",
    "    print(\"loss: {},\".format(np.mean(loss_list)),\n",
    "          \"accuracy: {}.\".format(np.mean(acc_list)))\n",
    "    labels_all = [w.item() for w in labels_all]\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average='weighted')\n",
    "    r = recall_score(labels_all, predict_all, average='weighted')\n",
    "    f1 = f1_score(labels_all, predict_all, average='weighted')\n",
    "    print(classification_report(labels_all, predict_all))\n",
    "    print('acc:', acc)\n",
    "    print('precision:', p)\n",
    "    print('recall:', r)\n",
    "    print('f1:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, text, tokenizer):\n",
    "    model = torch.load(\"bert_mini_sample_model.pth\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    time_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        text = text[:max_sequence_length - 2]\n",
    "        inputs = tokenizer.encode_plus(text,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=max_sequence_length,\n",
    "                                       return_tensors=\"pt\")\n",
    "        inputs = {\"input_ids\": inputs['input_ids'].to(device),\n",
    "                  \"attention_mask\": inputs['attention_mask'].to(device)}\n",
    "        logits = model(**inputs)\n",
    "        print(\"predict time cost {}\".format(time.time() - time_start))\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "    print(\"text: {}\".format(text))\n",
    "    print(\"predict label: {}\".format(predicted_class_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据: 200000 200000\n",
      "验证数据: 85187 85187\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(num_labels=3)\n",
    "train_x, train_y, eval_x, eval_y = load_raw_data(\"./data/review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据: 200000 200000\n",
      "验证数据: 85187 85187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc946dd45a8a4001bf0032072b230e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1/2, batch 500/3126, loss 0.5722352266311646, acc 0.7265625\n",
      "train epoch 1/2, batch 1000/3126, loss 0.5043110251426697, acc 0.78125\n",
      "train epoch 1/2, batch 1500/3126, loss 0.5758607983589172, acc 0.734375\n",
      "train epoch 2/2, batch 2000/3126, loss 0.3973851203918457, acc 0.84375\n",
      "train epoch 2/2, batch 2500/3126, loss 0.4306446611881256, acc 0.796875\n",
      "train epoch 2/2, batch 3000/3126, loss 0.3966715931892395, acc 0.828125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = data_loader(train_x,\n",
    "                               train_y,\n",
    "                               tokenizer,\n",
    "                               max_sequence_length,\n",
    "                               batch_size,\n",
    "                               True)\n",
    "train(model, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f890629b8fc4c30945bc30a8885dd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4445248742898305, accuracy: 0.8193493750280131.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85     28711\n",
      "           1       0.74      0.71      0.72     26437\n",
      "           2       0.88      0.87      0.87     30039\n",
      "\n",
      "    accuracy                           0.82     85187\n",
      "   macro avg       0.82      0.82      0.82     85187\n",
      "weighted avg       0.82      0.82      0.82     85187\n",
      "\n",
      "acc: 0.8193268926009837\n",
      "precision: 0.8186208921022787\n",
      "recall: 0.8193268926009837\n",
      "f1: 0.8187668699877619\n"
     ]
    }
   ],
   "source": [
    "# print(\"计算验证集的loss和acc～～～\")\n",
    "eval_dataloader = data_loader(eval_x,\n",
    "                              eval_y,\n",
    "                              tokenizer,\n",
    "                              max_sequence_length,\n",
    "                              batch_size,\n",
    "                              False)\n",
    "evaluate(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单条预测～～～\n",
      "predict time cost 0.009909868240356445\n",
      "text: Great Dj & dance music, food needs a bit of improvement as it lacks flavor\n",
      "predict label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"PREDICT \")\n",
    "text = \"Great Dj & dance music, food needs a bit of improvement as it lacks flavor\"\n",
    "predict(device, text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
